{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4594aeb0",
   "metadata": {},
   "source": [
    "# AARL Take-Home: Happy and Sad Faces\n",
    "\n",
    "This notebook describes a dataset consisting of 20 happy and sad faces, each of those represented by a $9 \\times 9$ image with only two colors, and a neural network trained to distinguish which ones are happy and sad. The neural network is trained by using only 10 of these images (the train set), with the purpose of correctly inferring which of the other ones are happy or sad (the test set).\n",
    "\n",
    "The development of training code and the creation of these images was done by Mikey Ferguson. The code for \"spying\" on the value of parameters of model was developed by Yuqin Yang and Thiago Serra. Thiago Serra has complemented this notebook with the present description and additional comments in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c942470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports some of the libraries that will be used\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch import relu, sigmoid, tanh, selu\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27faaaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "# This cell contains the 10 images that will be used for training\n",
    "\n",
    "x_train = torch.tensor(\n",
    "[\n",
    "    \n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "    \n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "    \n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "    \n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "    \n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "    \n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "    \n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
    "[0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "\n",
    "]\n",
    ")\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0223d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell tells which faces are happy or sad in the training set\n",
    "# The first five are happy, the next five are sad\n",
    "\n",
    "y_train = torch.tensor([0])\n",
    "# happy, unhappy\n",
    "y_train = y_train.reshape(-1, 1).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96553f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "# This cell contains the 10 images that will be used for testing\n",
    "# Like before, the first five are happy and the next five are sad\n",
    "\n",
    "x_test = torch.tensor(\n",
    "[\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0]],\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
    "[0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "],\n",
    "\n",
    "[    \n",
    "[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
    "[0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "[0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
    "[0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "]\n",
    "\n",
    "]\n",
    ")\n",
    "\n",
    "x_test = x_test.type(torch.FloatTensor)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce302e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we say which images in the test set are happy or sad\n",
    "\n",
    "y_test = torch.tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "y_test = y_test.view(-1,1).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055c818e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can use this cell to visualize each of these images\n",
    "plt.imshow(  x_test[0][0]  );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b963872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines the class of the neural network\n",
    "#  The architecture and how it process inputs is specified\n",
    "#  Unless stated othewise, all the comments below are Mikey's\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, convolutions = [(1, 2, 3)], hidden_layers = [98, 15, 15, 1]):\n",
    "        # hidden layers - the dimensions of each of the layers of our neural network\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        # now we can call all 'nn' methods and attributes\n",
    "        \n",
    "        self.convlist = convolutions\n",
    "        # EDITED THIS LINE\n",
    "        self.convolutions = nn.ModuleList()\n",
    "        for index, convolution in enumerate(convolutions):\n",
    "            \n",
    "            if index>0:\n",
    "                # then we have multiple convolutions - let's make sure the previous out_channels is consistent with our current number of in_channels\n",
    "                assert convolutions[index-1][1] == convolutions[index][0], 'Convolutions inconsistent - make sure that each successive pairing of output channels and input channels is consistent'\n",
    "            \n",
    "            in_channels = convolution[0]\n",
    "            out_channels = convolution[1]\n",
    "            kernel_size = convolution[2]\n",
    "            print(\"Kernel Size: \", kernel_size)\n",
    "            \n",
    "            self.convolutions.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            nn.init.kaiming_normal_(self.convolutions[-1].weight)\n",
    "            self.convolutions[-1].bias.data.fill_(0.01)\n",
    "            # nn.init.kaiming_normal_(self.convolutions[-1].bias)\n",
    "            # create a convolutional layer\n",
    "            \n",
    "        self.layer_dims = hidden_layers\n",
    "        self.linears = nn.ModuleList()\n",
    "        for this_dim, next_dim in zip(self.layer_dims, self.layer_dims[1:]):\n",
    "            self.linears.append(nn.Linear(this_dim, next_dim))\n",
    "            nn.init.kaiming_normal_(self.linears[-1].weight)\n",
    "            self.linears[-1].bias.data.fill_(0.01)\n",
    "            # nn.init.kaiming_normal_(self.linears[-1].bias)\n",
    "            # create all of our linear forward functions\n",
    "            \n",
    "        self.initialized = False\n",
    "        # we have not seen any images yet\n",
    "        \n",
    "        self.binary = hidden_layers[-1]==1\n",
    "        # does our model have a binary output?\n",
    "        \n",
    "        self.neurons = [None for i in range(len(self.linears))]\n",
    "        # to become a nested list of lists, each corresponding to the neuron activations of a given layer\n",
    "        # add 1 to the len(self.linears) because we have not yet created the function mapping the post-convoluted image to the first hidden layer\n",
    "            \n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.type(torch.FloatTensor)\n",
    "        # convert to float\n",
    "        \n",
    "        if not self.initialized:\n",
    "            # the network hasn't seen images yet - maybe they work with the current convolution setup, and maybe they don't...\n",
    "            if x.shape[1] != self.convlist[0][0]:\n",
    "                # the initial expected number of input channels\n",
    "                # then we're gonna need a new convolutional layer up front\n",
    "                new_conv = nn.Conv2d(in_channels = x.shape[1], out_channels = self.convlist[0][0], kernel_size=1)\n",
    "                self.convolutions.insert(0, new_conv)\n",
    "                # shove in this new convolutional layer at the beginning\n",
    "\n",
    "        # THESE ARE DEBUG STATEMENTS TO SEE CONVOLUTIONAL LAYER INPUTS AND VALUES\n",
    "        # print(\"input to conv\", x)\n",
    "        # print(\"self.convolutions\", self.convolutions)\n",
    "        \n",
    "        for convolution in self.convolutions:\n",
    "            x = convolution(x)\n",
    "\n",
    "            # THESE ARE DEBUG STATEMENTS TO SEE CONVOLUTIONAL LAYER OUTPUTS AND CONV LAYER\n",
    "            # print(\"Conv output\", x)\n",
    "            # print(\"conv layer\", convolution)\n",
    "            # apply each convolution\n",
    "        \n",
    "        # NOW... what on Earth could the dimensions of x be? Just have PyTorch figure it out...:-)\n",
    "        \n",
    "        num_pixels = 1\n",
    "        for shape in x.shape[1:]:\n",
    "            # I don't care about the first dimension (number of images) - this is only the number of pixels PER IMAGE\n",
    "            num_pixels *= shape\n",
    "        # total number of pixels\n",
    "        \n",
    "        if not self.initialized:\n",
    "            # again, the network hasn't seen any images yet\n",
    "            \n",
    "            if self.layer_dims[0] != num_pixels:\n",
    "                # then after our convolution, we are going to need to map from the input layer to the first hidden layer\n",
    "                # i.e., the number of pixels post-convlution did not correspond with the number of neurons in our first hidden layer specified by user input\n",
    "                \n",
    "                self.neurons.append(None)\n",
    "                # going to need another layer of neurons - keep track of that\n",
    "                \n",
    "                new_dim = x.view(-1, num_pixels)\n",
    "                # unroll into vector of dimension num_pixels\n",
    "\n",
    "                first_linear = nn.Linear(new_dim.shape[-1], self.layer_dims[0])\n",
    "                # num_pixels --> num_neurons in first hidden layer\n",
    "                self.linears.insert(0, first_linear)\n",
    "                # shove this linear function in at the beginning\n",
    "            \n",
    "            # otherwise, no need to create a whole new linear function\n",
    "        \n",
    "            self.initialized = True\n",
    "            # we have the first (potentially) required linear function and the first (potentially) required convolutional layer - do not create those again\n",
    "        \n",
    "        x = x.view(-1, num_pixels)\n",
    "        # reshape x so we can feed it through our network\n",
    "        \n",
    "        for index, linear_func in enumerate(self.linears):\n",
    "            # go through all of our linear functions\n",
    "            if index == len(self.linears) - 1:\n",
    "                # last layer\n",
    "                x = linear_func(x)\n",
    "                if self.binary:\n",
    "                    x = sigmoid(x)\n",
    "                    # binary output\n",
    "            else:\n",
    "                # not on the last layer\n",
    "                x = linear_func(x)\n",
    "                x = relu(x)\n",
    "                # relu activation\n",
    "            self.neurons[index]=(x.detach().tolist())\n",
    "            # keep track of the neuron activation\n",
    "        \n",
    "        return x\n",
    "    # return the output\n",
    "    \n",
    "    \n",
    "    def show_progression(self, x):\n",
    "        # x is just one image\n",
    "        \n",
    "        assert len(x.shape)==4 and x.shape[0]==1, \"show_progression method will only deal with one image at once - if you think you only have one, wrap it in an outer tensor.\"\n",
    "        assert self.initialized, \"Network has not yet been trained.\"\n",
    "        \n",
    "        print('Original Image:')\n",
    "        plt.imshow(x[0][0]) # Changed Mikey's code, which was: plt.imshow(x[0].permute(1,2,0))\n",
    "        plt.show()\n",
    "        \n",
    "        # editing (printing original value)\n",
    "        layer = x.detach().tolist()[0]\n",
    "        print(layer)\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "    \n",
    "        x = x.float()\n",
    "        # for some reason we need the preceding line of code if we're about to convolute; otherwise the convolution will hate us...\n",
    "        for index,convolutions in enumerate(self.convolutions):\n",
    "            # testing\n",
    "            print(x)\n",
    "            \n",
    "            print(f'Convolution {index + 1}:')\n",
    "            x = convolutions(x)\n",
    "            # apply the convolution\n",
    "            \n",
    "            # try to print the value (editing)\n",
    "            layer = x.detach().tolist()[0]\n",
    "            print(f'Convolution {index+1}:')\n",
    "            print(x.shape)\n",
    "            \n",
    "            #print([round(n, 5) for n in layer])\n",
    "            print(layer)\n",
    "            \n",
    "            copy = x.permute(1,0,2,3)\n",
    "            # turn it into the dimensions necessary in order to show the image via matplotlib.pyplot\n",
    "            for i in range(copy.shape[0]):\n",
    "                # sort of think of this as how many 'colors'/output channels\n",
    "                plt.imshow(copy.detach()[i][0]) # Changed Mikey's code, which was: plt.imshow(copy.detach()[i].permute(1,2,0))\n",
    "                plt.show()\n",
    "            print('\\n')\n",
    "        \n",
    "        num_pixels = 1\n",
    "        for shape in x.shape[1:]:\n",
    "            # I don't care about the first dimension (number of images) - this is only the number of pixels PER IMAGE\n",
    "            num_pixels *= shape\n",
    "        # total number of pixels\n",
    "        \n",
    "        x = x.view(-1, num_pixels)\n",
    "        print(x)\n",
    "        # reshape x so we can feed it through our network\n",
    "        \n",
    "        for index, linear_func in enumerate(self.linears):\n",
    "            # go through all of our linear functions\n",
    "            if index == len(self.linears) - 1:\n",
    "                # last layer\n",
    "                x = linear_func(x)\n",
    "                x = sigmoid(x)\n",
    "                # binary output\n",
    "            else:\n",
    "                # not on the last layer\n",
    "                x = linear_func(x)\n",
    "                x = relu(x)\n",
    "                # relu activation\n",
    "            layer = x.detach().tolist()[0]\n",
    "            print(f'Layer {index+1}:')\n",
    "            print([round(n, 5) for n in layer])\n",
    "            # print the (rounded) neuron activations\n",
    "            print('\\n')\n",
    "        \n",
    "        if self.binary:\n",
    "            print('Prediction:', torch.round(x).item())\n",
    "            # this is how you get either a zero or a 1\n",
    "        else:\n",
    "            print('Prediction:', torch.max(x, axis=1)[1].item())\n",
    "            # print the index of the maximum value in the output tensor (the classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52e4bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Size:  3\n",
      "tensor([[0.5196],\n",
      "        [0.5981],\n",
      "        [0.4696],\n",
      "        [0.4978],\n",
      "        [0.5107],\n",
      "        [0.4742],\n",
      "        [0.6208],\n",
      "        [0.4803],\n",
      "        [0.3853],\n",
      "        [0.5269]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now we create an object based on the neural network class above\n",
    "\n",
    "model = CNN()\n",
    "print(model(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21882865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines the function for training the neural network\n",
    "#  If we were to \"spy\" on how the parameters of the neural network change over time,\n",
    "#  This would be the place to do it\n",
    "#  Unless stated otherwise, all the comments below are Mikey's\n",
    "\n",
    "def train(model, x_train, y_train, x_test, y_test, num_epochs, lr, show_wrong=False):\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    # log of the probability\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    # responsible for updating our convolutional layer, weight matrices, and bias vectors\n",
    "    \n",
    "    checkpoints = [i for i in range(num_epochs) if i % (num_epochs // 10) == 0]\n",
    "    # 0%, 10%, 20%, etc. through training\n",
    "    \n",
    "    LOSS = []\n",
    "    ACCURACY = []\n",
    "    # keep track of these by the epoch\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # go through all of our data this many times\n",
    "        \n",
    "        yhat = model(x_train)\n",
    "        # obtain model prediction\n",
    "        yhat = yhat.view(-1,1)\n",
    "        \n",
    "        loss = criterion(yhat, y_train)\n",
    "        # compute the loss\n",
    "        \n",
    "        LOSS.append(loss.data)\n",
    "        # keep track of the epoch loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # refresh gradient\n",
    "        \n",
    "        loss.backward()\n",
    "        # compute gradient\n",
    "        \n",
    "        # TS: At this point the gradient has been computed,\n",
    "        #  but the parameters have not been updated yet\n",
    "        \n",
    "        optimizer.step()\n",
    "        # update parameters\n",
    "        \n",
    "        # TS: At this point the parameters have been updated\n",
    "        \n",
    "        # NOW check the accuracy\n",
    "        \n",
    "        yhat_test = model(x_test)\n",
    "        # obtain test data prediction\n",
    "        yhat_test = yhat_test.view(-1,1)\n",
    "        \n",
    "        yhat_test = torch.round(yhat_test)\n",
    "        # round to 0, 1 --> binary output\n",
    "       \n",
    "        num_correct = torch.sum(yhat_test==y_test)\n",
    "        accuracy = num_correct / len(y_test)\n",
    "        # count the number of testing inputs our model got correct\n",
    "        \n",
    "        ACCURACY.append(accuracy)\n",
    "        # keep track of this epoch's accuracy\n",
    "        \n",
    "        # are we at a checkpoint?\n",
    "        if epoch in checkpoints:\n",
    "            print(f'{100*epoch//num_epochs}% through training...')\n",
    "            \n",
    "    # now all of our training is concluded\n",
    "\n",
    "    print('loss data', LOSS)\n",
    "    plt.plot(LOSS, 'rx')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(ACCURACY, 'go')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    if show_wrong:\n",
    "        print('\\n')\n",
    "        print('Incorrect Train Classifications:')\n",
    "    \n",
    "    train_incorrect = []\n",
    "    # let's keep track of the images which our model could learn to classify correctly\n",
    "    for image, classification in zip(x_train, y_train):\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "        z = model(image)\n",
    "        yhat = torch.round(z)\n",
    "        # 1D tensor because of batch size\n",
    "        \n",
    "        yhat=yhat.view(-1,1)\n",
    "        y=classification.view(-1,1)\n",
    "        \n",
    "        if yhat.item()!=y.item():\n",
    "            # then the trained model still cannot grasp this image\n",
    "            train_incorrect.append(image)\n",
    "            \n",
    "            if show_wrong:\n",
    "                plt.imshow(image[0].permute(1,2,0))\n",
    "                plt.show()\n",
    "                print(classification.item())\n",
    "    \n",
    "    if show_wrong:\n",
    "        print('\\n')\n",
    "        print('Incorrect Test Classifications:')\n",
    "        \n",
    "    test_incorrect = []\n",
    "    # let's keep track of the images which our model failed to classify correctly on the spot\n",
    "    for image, classification in zip(x_test, y_test):\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "        z = model(image)\n",
    "        yhat = torch.round(z)\n",
    "        # 1D tensor because of batch size\n",
    "        \n",
    "        yhat=yhat.view(-1,1)\n",
    "        y=classification.view(-1,1)\n",
    "        \n",
    "        if yhat.item()!=y.item():\n",
    "            # then the model failed to classify this image\n",
    "            test_incorrect.append(image)\n",
    "            \n",
    "            if show_wrong:\n",
    "                plt.imshow(image[0].permute(1,2,0))\n",
    "                plt.show()\n",
    "                print(classification.item())\n",
    "    \n",
    "    if show_wrong:\n",
    "        print('\\n')\n",
    "        \n",
    "    return train_incorrect, test_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7aeb3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Size:  3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Now we will call the function above for training it\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_incorrect, test_incorrect \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, x_train, y_train, x_test, y_test, num_epochs, lr, show_wrong)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# obtain model prediction\u001b[39;00m\n\u001b[0;32m     26\u001b[0m yhat \u001b[38;5;241m=\u001b[39m yhat\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43myhat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# compute the loss\u001b[39;00m\n\u001b[0;32m     31\u001b[0m LOSS\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3087\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3089\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3092\u001b[0m     )\n\u001b[0;32m   3094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3095\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Now we are training the neural network\n",
    "\n",
    "# Here we define hyperparameters about training\n",
    "num_epochs = 25\n",
    "lr = 0.01\n",
    "# Here we create a new object for the neural network\n",
    "model = CNN()\n",
    "\n",
    "# Now we will call the function above for training it\n",
    "train_incorrect, test_incorrect = train(model, x_train, y_train, x_test, y_test, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are curious about which tests failed, we can look them up\n",
    "\n",
    "if len(test_incorrect)>0:\n",
    "    # This will show only the first test that was incorrect, in case there is one\n",
    "    model.show_progression(test_incorrect[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc063b9",
   "metadata": {},
   "source": [
    "## Looking into the parameters of the model\n",
    "\n",
    "What if we wanted to spy on the model, to see how its parameters evolve over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d579aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With this piece of code, we can extract the parameters of the linear layers from the model\n",
    "#  The parameters for each layer are saved in a different file\n",
    "\n",
    "for i in range(3): # There are exactly three layers\n",
    "    my_array = model.state_dict()['linears.'+str(i)+'.weight'].numpy()\n",
    "    count = 0\n",
    "    f = open(\"layer\"+str(i+1)+\".csv\", \"w\")\n",
    "    for array in my_array:\n",
    "        f.write(str(count) + ',')\n",
    "        for i in array:\n",
    "            f.write(str(i) +  \", \")      \n",
    "        f.write('\\n')    \n",
    "        count += 1\n",
    "        # each represents a neuron\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184424eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this piece of code, we can extract the parameters of the convolutional layer from the model\n",
    "#  One file is used for saving the weights\n",
    "#  Another file is used for saving the biases\n",
    "\n",
    "conv = model.convolutions[0] # there is only one convolution\n",
    "f = open(\"conv.weight(OLD).csv\", \"w\")\n",
    "for a in range(2):\n",
    "    for i in range(3):\n",
    "        for b in range(3):\n",
    "            f.write(str(conv.weight[a][0][i][b].item()) + \", \")\n",
    "f.close()\n",
    "    \n",
    "f = open(\"conv.bias(OLD).csv\", \"w\")\n",
    "for i in range(2):\n",
    "    f.write(str(conv.bias[i].item()) + \", \")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe15461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look into the weights and the gradient of the last linear layer as below\n",
    "\n",
    "print(model.linears[-1].weight.grad)\n",
    "print(model.linears[-1].weight)\n",
    "# print(len(model.linears))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08045b",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6dd14",
   "metadata": {},
   "source": [
    "### 1: More information and better output about the parameters of the trained model\n",
    "\n",
    "Note that the biases of the linear layers are not recorded in the cell that saves the weights of the linear layers in CSV files. In that file, the first record in each line is the number of the neuron, starting at zero. Immediately following that, we have the parameters. \n",
    "\n",
    "Can you copy that code below and modify it in such a way that:\n",
    "\n",
    "1) There is an empty record between the number of the neuron and the weigths. For example, a line such as \n",
    "\n",
    "0, 1.3, 2.3, 4.5,\n",
    "\n",
    "would instead look as follows:\n",
    "\n",
    "0, , 1.3, 2.3, 4.5,\n",
    "\n",
    "2) There is another empty record following the weights, which is followed by the bias of the corresponding neuron. For example, the line above may look like this if the bias for that neuron is -0.3:\n",
    "\n",
    "0, , 1.3, 2.3, 4.5, , -0.3,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee38800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here! :-)\n",
    "for i in range(3): # There are exactly three layers\n",
    "    my_array = model.state_dict()['linears.'+str(i)+'.weight'].numpy()\n",
    "    bias_array = model.state_dict()['linears.'+str(i)+'.bias'].numpy()\n",
    "    count = 0\n",
    "    f = open(\"layer\"+str(i+1)+\".csv\", \"w\")\n",
    "    for array in my_array:\n",
    "        # This portion was modified by adding a \" ,\" string to separate neuron number and parameters\n",
    "        f.write(str(count) + ',' + \" ,\")\n",
    "        # This portion was modified by using the indices of the length/shape of the array.\n",
    "        for i in range(len(array)):\n",
    "            f.write(str(array[i]) +  \", \")\n",
    "        f.write(\", \" + str(bias_array[count]))\n",
    "        f.write('\\n')    \n",
    "        count += 1\n",
    "        # each represents a neuron\n",
    "    f.close()\n",
    "    # print(\"array: \", array)\n",
    "    # print(\"Bias: \", bias_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b55e3",
   "metadata": {},
   "source": [
    "Moreover, if you look into the file produced with parameters of the convolutional layer, notice that they are in a single line. Not only that, but the two filters are not separated: it is all in that same line. Each filter consists of a square matrix of parameters.\n",
    "\n",
    "Can you copy that code below and modify it in such a way that:\n",
    "\n",
    "1) Each filter is recorded in a separate CSV file. Each file should have a square matrix with its parameters.\n",
    "\n",
    "2) The bias is recorded as a single record below the filter, but separated from it with a line containing a single comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4a21a-8b19-44b7-b3fb-38fb0bcc7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here! :-)\n",
    "conv = model.convolutions[0] # there is only one convolution\n",
    "# f = open(\"conv.weight.csv\", \"w\")\n",
    "# print(conv.kernel_size)\n",
    "# filterSize = fRow * fColumn\n",
    "# print(len(conv.weight))\n",
    "\n",
    "# This function separates the filters into their own files and restructures the parameters into square matrices. It also places the bias below the filter.\n",
    "fRow, fColumn = conv.kernel_size\n",
    "numFilters = len(conv.weight)\n",
    "for filter in range(numFilters):\n",
    "    f = open(\"conv.weight_bias.filter\" + str(filter) + \".csv\", \"w\")\n",
    "    for i in range(fRow):\n",
    "        for b in range(fColumn):\n",
    "            f.write(str(conv.weight[filter][0][i][b].item()) + \", \")\n",
    "        f.write('\\n')\n",
    "    f.write(\", \")\n",
    "    f.write('\\n')\n",
    "    f.write(str(conv.bias[filter].item()) + \", \")\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988e39c",
   "metadata": {},
   "source": [
    "## 2: Tracking all parameter updates\n",
    "\n",
    "What if we wanted to look at what happened in every step of the training? For example, by recordign all the gradients and all the parameters before, during, and after the training is over. In other words, for every update of the parameters, you save in file what was the gradient update and what is the new value of the parameters.\n",
    "\n",
    "For this exercise, please note the following:\n",
    "\n",
    "1) Because you will be changing a lot of things, please make sure to put comments along the code explaining what you changed\n",
    "\n",
    "2) Because many files will be create, please make sure to create a system for that purpose: file names numbered from 0, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here! :-)\n",
    "\n",
    "# NOTE: I WROTE ALL OF MY COMMENTS IN CAPS\n",
    "def train(model, x_train, y_train, x_test, y_test, num_epochs, lr, show_wrong=False):\n",
    "\n",
    "    print('initial sigmoid', model(x_train))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    # log of the probability\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    # responsible for updating our convolutional layer, weight matrices, and bias vectors\n",
    "    \n",
    "    checkpoints = [i for i in range(num_epochs) if i % (num_epochs // 10) == 0]\n",
    "    # 0%, 10%, 20%, etc. through training\n",
    "    \n",
    "    LOSS = []\n",
    "    ACCURACY = []\n",
    "    # keep track of these by the epoch\n",
    "\n",
    "    # RECORDING PARAMETER VALUES BEFORE TRAINING\n",
    "    # ====================================================================================\n",
    "    # LINEAR LAYER WEIGHTS AND BIASES (2 LAYERS ONLY?)\n",
    "    # WHY ARE THERE ONLY 2 LAYERS BEFORE TRAINING?\n",
    "    # IT SEEMS THAT THESE ARE THE SECOND AND THIRD LAYERS (HENCE WHY I ADDED 1 TO LAYERS BELOW TO THE FILENAME)\n",
    "    print(\"linear layers\" + str(len(model.linears)))\n",
    "    for layers in range(len(model.linears)):\n",
    "        path = os.path.join(\"training_data\", \"linearLayer\" + str(layers) + \"BEFORE.csv\")\n",
    "        f = open(path, \"w\")\n",
    "        for i in range(len(model.linears[layers].weight)):\n",
    "            for j in range(len(model.linears[layers].weight[i])):\n",
    "                f.write(str(model.linears[layers].weight[i][j].item()) + \", \")\n",
    "            f.write(\", \")\n",
    "            f.write(str(model.linears[layers].bias[i].item()) + \", \")\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "    \n",
    "    # GRADIENTS\n",
    "    # THERE IS NO BEFORE GRADIENT AS NO TRAINING HAS BEEN DONE TO OPTIMIZE THE VALUES OF THE PARAMETERS/BIASES.\n",
    "\n",
    "    # CONVOLUTION LAYER PARAMETER VALUES\n",
    "    conv = model.convolutions[0] # there is only one convolution\n",
    "    # This function separates the filters into their own files and restructures the parameters into square matrices. It also places the bias below the filter.\n",
    "    fRow, fColumn = conv.kernel_size\n",
    "    numFilters = len(conv.weight)\n",
    "    print(\"FILTER NUMBER\", numFilters)\n",
    "    for filter in range(numFilters):\n",
    "        path = os.path.join(\"training_data\", \"convParam\" + str(filter) + \"BEFORE.csv\")\n",
    "        f = open(path, \"w\")\n",
    "        for i in range(fRow):\n",
    "            for b in range(fColumn):\n",
    "                f.write(str(conv.weight[filter][0][i][b].item()) + \", \")\n",
    "            f.write('\\n')\n",
    "        f.write(\", \")\n",
    "        f.write('\\n')\n",
    "        f.write(str(conv.bias[filter].item()) + \", \")\n",
    "    f.close()\n",
    "\n",
    "    # ====================================================================================\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # go through all of our data this many times\n",
    "        \n",
    "        yhat = model(x_train)\n",
    "        # obtain model prediction\n",
    "\n",
    "        # THE LINE BELOW PRINTS THE SIGMOID VAL FOR EVERY TRAINING EPOCH\n",
    "        print('sigmoid val', yhat)\n",
    "        yhat = yhat.view(-1,1)\n",
    "        \n",
    "        loss = criterion(yhat, y_train)\n",
    "        # compute the loss\n",
    "        \n",
    "        LOSS.append(loss.data)\n",
    "        # keep track of the epoch loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # refresh gradient\n",
    "        \n",
    "        loss.backward()\n",
    "        # compute gradient\n",
    "        \n",
    "        # TS: At this point the gradient has been computed,\n",
    "        #  but the parameters have not been updated yet\n",
    "        \n",
    "        optimizer.step()\n",
    "        # update parameters\n",
    "\n",
    "        \n",
    "        # print(model.state_dict()['linears.'+str(0)+'.weight'].numpy())\n",
    "        \n",
    "        # TS: At this point the parameters have been updated\n",
    "        \n",
    "        # NOW check the accuracy\n",
    "        \n",
    "        yhat_test = model(x_test)\n",
    "        # obtain test data prediction\n",
    "        yhat_test = yhat_test.view(-1,1)\n",
    "\n",
    "    \n",
    "        \n",
    "        yhat_test = torch.round(yhat_test)\n",
    "        # round to 0, 1 --> binary output\n",
    "       \n",
    "        num_correct = torch.sum(yhat_test==y_test)\n",
    "        accuracy = num_correct / len(y_test)\n",
    "        # count the number of testing inputs our model got correct\n",
    "        \n",
    "        ACCURACY.append(accuracy)\n",
    "        # keep track of this epoch's accuracy\n",
    "        \n",
    "        # are we at a checkpoint?\n",
    "        if epoch in checkpoints:\n",
    "            print(f'{100*epoch//num_epochs}% through training...')\n",
    "\n",
    "        # TRACKING PARAMETERS AND GRADIENT DURING TRAINING HERE\n",
    "        # ====================================================================================\n",
    "        # LINEAR LAYER WEIGHTS AND BIASES\n",
    "        print(\"linear layers\" + str(len(model.linears)))\n",
    "        for layers in range(len(model.linears)):\n",
    "            path = os.path.join(\"training_data\", \"linearLayer\" + str(layers) + \"_\" + str(epoch) + \".csv\")\n",
    "            f = open(path, \"w\")\n",
    "            for i in range(len(model.linears[layers].weight)):\n",
    "                for j in range(len(model.linears[layers].weight[i])):\n",
    "                    f.write(str(model.linears[layers].weight[i][j].item()) + \", \")\n",
    "                f.write(\", \")\n",
    "                f.write(str(model.linears[layers].bias[i].item()) + \", \")\n",
    "                f.write('\\n')\n",
    "            f.close()\n",
    "\n",
    "        # GRADIENTS LINEAR\n",
    "        for layers in range(len(model.linears)):\n",
    "            path = os.path.join(\"training_data\", \"lineargrad\" + str(layers) + \"_\" + str(epoch) + \".csv\")\n",
    "            f = open(path, \"w\")\n",
    "            for node in range(len(model.linears[layers].weight.grad)):\n",
    "                for i in range(len(model.linears[layers].weight.grad[node])):\n",
    "                    # for j in range(len(model.linears[layers].weight[i])):\n",
    "                    f.write(str(model.linears[layers].weight.grad[node][i].item()) + \", \")\n",
    "                f.write(\", \")\n",
    "                f.write(str(model.linears[layers].bias.grad[node].item()))\n",
    "                f.write('\\n')\n",
    "            f.close()\n",
    "            \n",
    "\n",
    "        # CONVOLUTION LAYER PARAMETERS\n",
    "        # DOES NOT CHANGE FOR SOME REASON.\n",
    "        conv = model.convolutions[0] # there is only one convolution\n",
    "        # This function separates the filters into their own files and restructures the parameters into square matrices. It also places the bias below the filter.\n",
    "        fRow, fColumn = conv.kernel_size\n",
    "        numFilters = len(conv.weight)\n",
    "        for filter in range(numFilters):\n",
    "            path = os.path.join(\"training_data\", \"convParam\" + str(filter) + \"_\" + str(epoch) + \".csv\")\n",
    "            f = open(path, \"w\")\n",
    "            for i in range(fRow):\n",
    "                for b in range(fColumn):\n",
    "                    f.write(str(conv.weight[filter][0][i][b].item()) + \", \")\n",
    "                f.write('\\n')\n",
    "            f.write(\", \")\n",
    "            f.write('\\n')\n",
    "            f.write(str(conv.bias[filter].item()) + \", \")\n",
    "        f.close()\n",
    "\n",
    "        # CONVOLUTION LAYER GRADIENTS\n",
    "        for filter in range(numFilters):\n",
    "            path = os.path.join(\"training_data\", \"convGrad\" + str(filter) + \"_\" + str(epoch) + \".csv\")\n",
    "            f = open(path, \"w\")\n",
    "            for i in range(fRow):\n",
    "                for b in range(fColumn):\n",
    "                    f.write(str(conv.weight.grad[filter][0][i][b].item()) + \", \")\n",
    "                f.write('\\n')\n",
    "            f.write('\\n')\n",
    "            f.write(str(conv.bias.grad[filter].item()))\n",
    "        f.close()\n",
    "        \n",
    "        # ====================================================================================\n",
    "\n",
    "    # FINAL VALUES\n",
    "    # ====================================================================================\n",
    "    # LINEAR LAYER PARAMS\n",
    "    # THE FINAL WEIGHTS AND BIASES DATA IS THE LAST CSV FILE FOR WEIGHT.\n",
    "    \n",
    "    # GRADIENTS\n",
    "    # THE FINAL LINEAR AND CL GRADIENT DATA IS THE LAST CSV FILE FOR LINEARGRAD AND CONVGRAD.\n",
    "\n",
    "    # CONVOLUTIONAL LAYER PARAMS\n",
    "    # THE FINAL CL WEIGHTS AND BIASES DATA IS THE LAST CSV FILES FOR CONVPARAMS.\n",
    "\n",
    "    # ====================================================================================\n",
    "    \n",
    "    # now all of our training is concluded\n",
    "\n",
    "    print('loss data', LOSS)\n",
    "    plt.plot(LOSS, 'rx')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(ACCURACY, 'go')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    if show_wrong:\n",
    "        print('\\n')\n",
    "        print('Incorrect Train Classifications:')\n",
    "    \n",
    "    train_incorrect = []\n",
    "    # let's keep track of the images which our model could learn to classify correctly\n",
    "    for image, classification in zip(x_train, y_train):\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "        z = model(image)\n",
    "        yhat = torch.round(z)\n",
    "        # 1D tensor because of batch size\n",
    "        \n",
    "        yhat=yhat.view(-1,1)\n",
    "        y=classification.view(-1,1)\n",
    "        \n",
    "        if yhat.item()!=y.item():\n",
    "            # then the trained model still cannot grasp this image\n",
    "            train_incorrect.append(image)\n",
    "            \n",
    "            if show_wrong:\n",
    "                plt.imshow(image[0].permute(1,2,0))\n",
    "                plt.show()\n",
    "                print(classification.item())\n",
    "    \n",
    "    if show_wrong:\n",
    "        print('\\n')\n",
    "        print('Incorrect Test Classifications:')\n",
    "        \n",
    "    test_incorrect = []\n",
    "    # let's keep track of the images which our model failed to classify correctly on the spot\n",
    "    for image, classification in zip(x_test, y_test):\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "        z = model(image)\n",
    "        yhat = torch.round(z)\n",
    "        # 1D tensor because of batch size\n",
    "        \n",
    "        yhat=yhat.view(-1,1)\n",
    "        y=classification.view(-1,1)\n",
    "        \n",
    "        if yhat.item()!=y.item():\n",
    "            # then the model failed to classify this image\n",
    "            test_incorrect.append(image)\n",
    "            \n",
    "            if show_wrong:\n",
    "                plt.imshow(image[0].permute(1,2,0))\n",
    "                plt.show()\n",
    "                print(classification.item())\n",
    "    \n",
    "    if show_wrong:\n",
    "        print('\\n')\n",
    "        \n",
    "    return train_incorrect, test_incorrect\n",
    "\n",
    "# TESTING OF PARAMETER UPDATE TRACKING\n",
    "\n",
    "# Here we define hyperparameters about training\n",
    "num_epochs = 25\n",
    "lr = 0.01\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Here we create a new object for the neural network\n",
    "model = CNN()\n",
    "\n",
    "# ADDED THIS LINE\n",
    "# model(x_test)\n",
    "\n",
    "# Now we will call the function above for training it\n",
    "train_incorrect, test_incorrect = train(model, x_train, y_train, x_test, y_test, num_epochs, lr)\n",
    "\n",
    "# PRINT STATEMENTS TO TEST AND UNDERSTAND THE ML MODEL.\n",
    "# print(num_epochs)\n",
    "# print(len(model.linears))\n",
    "# print(model.linears[0].weight.grad)\n",
    "# print(len(model.linears[0].weight.grad[0]))\n",
    "# print(model.linears[0].weight)\n",
    "# print(len(model.linears[0].weight))\n",
    "# print(model.parameters)\n",
    "# print(model.linears[0].weight[0][0])\n",
    "# print(type(model.linears[0].weight[0][0]))\n",
    "# print(model.linears[0].bias)\n",
    "# print(model.linears[1].bias)\n",
    "# print(model.linears[2].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfe48a",
   "metadata": {},
   "source": [
    "## 3: Anything else?\n",
    "\n",
    "You are welcome, but not expected, to implement anything else based on the code above that you may find interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51dc32-85ec-435d-9431-46d2b05feb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING FOR DIMENSION OF ARRAYS\n",
    "print(len(conv.weight.grad[0]))\n",
    "print(len(model.linears[0].weight.grad))\n",
    "print(dir(model.convolutions[0]))\n",
    "print(conv.weight.grad)\n",
    "print(conv.weight[0][0][0][0])\n",
    "print(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dedaf7-585e-4e38-9b23-59f52ee12392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE THAT DOES NOT SEEM TO UPDATE PARAMETERS (OLD CODE)\n",
    "\n",
    "# BEFORE TRAINING CODE ------------------------------------------------------------------------------------------------\n",
    "# for array in my_array:\n",
    "        # # This portion was modified by adding a \" ,\" string to separate neuron number and parameters\n",
    "        #     f.write(str(count) + ',' + \" ,\")\n",
    "        #     # This portion was modified by using the indices of the length/shape of the array.\n",
    "        #     for i in range(len(array)):\n",
    "        #         f.write(str(array[i]) +  \", \")\n",
    "        #     f.write(\", \" + str(bias_array[count]))\n",
    "        #     f.write('\\n')\n",
    "        #     count += 1\n",
    "        # f.close()\n",
    "\n",
    "    # # WEIGHTS\n",
    "    # path = os.path.join(\"training_data\", \"weight\" +\"BEFORE.csv\")\n",
    "    # f = open(path, \"w\")\n",
    "    # for layers in range(len(model.linears)):\n",
    "    #     for node in range(len(model.linears[layers].weight[0])):\n",
    "    #         f.write(str(model.linears[layers].weight[0][node]) + \", \")\n",
    "    #     f.write('\\n')\n",
    "    # f.close()\n",
    "    # # THIS WILL REMOVE THE UNNECESSARY \", grad_fn=<SelectBackward0>)\" VALUES\n",
    "    # stringToDelete = \", grad_fn=<SelectBackward0>)\"\n",
    "    # tempFile = os.path.join(\"training_data\", \"weight\" +\"BEFOREtemp.csv\")\n",
    "    # with open(path, \"r\") as input:\n",
    "    #     with open(tempFile, \"w\") as output:\n",
    "    #         for line in input:\n",
    "    #             line = line.replace(stringToDelete, \"\")\n",
    "    #             output.write(line)\n",
    "    # # replace file with original name\n",
    "    # os.replace(tempFile, path)\n",
    "    # # BIASES\n",
    "    # path = os.path.join(\"training_data\", \"bias\" + \"BEFORE.csv\")\n",
    "    # f = open(path, \"w\")\n",
    "    # for layers in range(len(model.linears)):\n",
    "    #     for node in range(len(model.linears[layers].bias)):\n",
    "    #         f.write(str(model.linears[layers].bias[node]) + \", \")\n",
    "    #     f.write('\\n')\n",
    "    # f.close()\n",
    "    # # THIS WILL REMOVE THE UNNECESSARY \", grad_fn=<SelectBackward0>)\" VALUES\n",
    "    # stringToDelete = \", grad_fn=<SelectBackward0>)\"\n",
    "    # tempFile = os.path.join(\"training_data\", \"bias\" + \"BEFOREtemp.csv\")\n",
    "    # with open(path, \"r\") as input:\n",
    "    #     with open(tempFile, \"w\") as output:\n",
    "    #         for line in input:\n",
    "    #             line = line.replace(stringToDelete, \"\")\n",
    "    #             output.write(line)\n",
    "    # # replace file with original name\n",
    "    # os.replace(tempFile, path)\n",
    "\n",
    "\n",
    "\n",
    "# DURING TRAINING CODE ---------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# print(\"Model's state_dict:\")\n",
    "        # for param_tensor in model.state_dict():\n",
    "        #     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "        \n",
    "        # print()\n",
    "        \n",
    "        # for i in range(len(model.linears)): \n",
    "        #     my_array = model.state_dict()['linears.'+str(i)+'.weight.grad'].numpy()\n",
    "        #     count = 0\n",
    "        #     path = os.path.join(\"training_data\", \"linearLayer\" + str(i) + \"Grad_\" + str(epoch) + \".csv\")\n",
    "        #     f = open(path, \"w\")\n",
    "        #     for array in my_array:\n",
    "        #     # This portion was modified by adding a \" ,\" string to separate neuron number and parameters\n",
    "        #         f.write(str(count) + ',' + \" ,\")\n",
    "        #         # This portion was modified by using the indices of the length/shape of the array.\n",
    "        #         for i in range(len(array)):\n",
    "        #             f.write(str(array[i]) +  \", \")\n",
    "        #         f.write('\\n')\n",
    "        #         count += 1\n",
    "        #     f.close()\n",
    "            \n",
    "        # path = os.path.join(\"training_data\", \"lineargrad\" + str(epoch) +\".csv\")\n",
    "        # f = open(path, \"w\")\n",
    "        # for layers in range(len(model.linears)):\n",
    "        #     for node in range(len(model.linears[layers].weight.grad)):\n",
    "        #         for grad in range(len(model.linears[layers].weight.grad[node])):\n",
    "        #             f.write(str(model.linears[layers].weight.grad[node][grad]) + \", \")\n",
    "        #     f.write('\\n')\n",
    "        # f.close()\n",
    "\n",
    "# for i in range(len(model.linears)): \n",
    "        #     my_array = model.state_dict()['linears.'+str(i)+'.weight'].numpy()\n",
    "        #     bias_array = model.state_dict()['linears.'+str(i)+'.bias'].numpy()\n",
    "        #     count = 0\n",
    "        #     path = os.path.join(\"training_data\", \"linearLayer\" + str(i) + \"_\" + str(epoch) + \".csv\")\n",
    "        #     f = open(path, \"w\")\n",
    "        #     for array in my_array:\n",
    "        #     # This portion was modified by adding a \" ,\" string to separate neuron number and parameters\n",
    "        #         f.write(str(count) + ',' + \" ,\")\n",
    "        #         # This portion was modified by using the indices of the length/shape of the array.\n",
    "        #         for i in range(len(array)):\n",
    "        #             f.write(str(array[i]) +  \", \")\n",
    "        #         f.write(\", \" + str(bias_array[count]))\n",
    "        #         f.write('\\n')\n",
    "        #         count += 1\n",
    "        #     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f6bd1-24f5-4a71-88a2-98fdaff87d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITED (10/23/2023)\n",
    "# -ADDED LINE IN CNN class init FUNCTION\n",
    "# -ADDED LINE IN TRAINING CELL FUNCTION BEFORE THE TRAINING FUNCTION is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34df40-69dd-48c2-9399-16165c5b0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11/6/2023\n",
    "# LOOK INTO OUTPUT VALUES FOR CONVOLUTIONAL LAYERS AND EVERY LINEAR LAYERS\n",
    "# X IN EXCEL FILE IS WHAT GETS FED INTO THE NEXT LINEAR LAYERS\n",
    "# LOOK INTO BCELOSS function and how to implement it in excel to compare grads/parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d08e36-717e-4f73-8d2d-774f1a694f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11/8/2023\n",
    "\n",
    "# Look into BCELoss function in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d0828-0358-4c35-b74b-1c1088f5d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "\n",
    "help(torch.optim.Adam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
